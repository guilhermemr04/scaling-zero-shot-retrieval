{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BEIR.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE7wYCuL8bUW"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall git+https://github.com/castorini/pygaggle\n",
        "!pip install faiss-cpu==1.7.2 --quiet\n",
        "!pip install jsonlines==3.0.0 --quiet\n",
        "!pip install beir==1.0.0 --quiet\n",
        "!pip install protobuf==3.20.1 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available(): \n",
        "    dev = \"cuda:0\"\n",
        "    print(dev, torch.cuda.get_device_name(0))\n",
        "    device = torch.device(dev)\n",
        "else: \n",
        "    dev = \"cpu\"\n",
        "    print(dev) \n",
        "\n",
        "from pyserini.search import SimpleSearcher\n",
        "from pygaggle.rerank.base import hits_to_texts\n",
        "from pygaggle.rerank.base import Query, Text\n",
        "from pygaggle.rerank.transformer import MonoT5\n",
        "from transformers import T5ForConditionalGeneration\n",
        "from pygaggle.rerank.transformer import SentenceTransformersReranker\n",
        "import jsonlines\n",
        "import os\n",
        "import math\n",
        "from tqdm.notebook import tqdm\n",
        "from beir import util\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "uEd7a9Wg8dKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset(dataset):\n",
        "    \"\"\"\n",
        "    Download a BEIR dataset (test set only). Return the preprocessed corpus, queries and qrels\n",
        "\n",
        "    Args:\n",
        "      dataset: Dataset name (string)\n",
        "\n",
        "    Returns:  \n",
        "      Return the preprocessed corpus, queries and qrels\n",
        "    \"\"\"\n",
        "    print('Downloading', dataset)\n",
        "    url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
        "    data_path = util.download_and_unzip(url, \"datasets\")\n",
        "    corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
        "\n",
        "    return corpus, queries, qrels\n",
        "\n",
        "\n",
        "def prepare_qrels(dataset):\n",
        "    \"\"\"\n",
        "    Convert qreld to TREC eval format\n",
        "\n",
        "    Args:\n",
        "      dataset: Dataset name (string)\n",
        "\n",
        "    \"\"\"\n",
        "    df_qrel = pd.read_csv('/content/datasets/{}/qrels/test.tsv'.format(dataset), sep='\\t')\n",
        "    df_qrel['zero'] = '0'\n",
        "    cols = ['query-id', 'zero',\t'corpus-id', 'score']\n",
        "    df_qrel = df_qrel[cols] \n",
        "    df_qrel.to_csv('qrel.tsv', sep='\\t', header = None, index = False)\n",
        "\n",
        "\n",
        "def index_corpus(corpus):\n",
        "    \"\"\"\n",
        "    Index corpus to be retrieved by BM25\n",
        "\n",
        "    Args:\n",
        "      corpus: Corpus (dict)\n",
        "      \n",
        "    Returns:\n",
        "      Searcher object to initialize BM25\n",
        "    \"\"\"\n",
        "\n",
        "    !rm -r candidates\n",
        "    !rm -r tmp_candidates\n",
        "    !mkdir tmp_candidates\n",
        "\n",
        "    for key, val in tqdm(corpus.items()):  \n",
        "        indexed_dict = { \"id\": str(key), \"contents\": val['title'] + ' ' + val['text']}\n",
        "        with jsonlines.open('/content/tmp_candidates/candidate.jsonl', mode='a') as writer:\n",
        "            writer.write(indexed_dict)\n",
        "         \n",
        "    !python -m pyserini.index -collection JsonCollection -generator DefaultLuceneDocumentGenerator \\\n",
        "    -threads 1 -input /content/tmp_candidates \\\n",
        "    -index /content/candidates/indexes -storePositions -storeDocvectors -storeRaw\n",
        "\n",
        "    searcher = SimpleSearcher('/content/candidates/indexes')\n",
        "\n",
        "    return searcher\n",
        "\n",
        "\n",
        "def run_retrieval(queries, searcher, model_name):\n",
        "    \"\"\"\n",
        "    Run BM25 and reranker retrieval. Save the outputs as txt files\n",
        "\n",
        "    Args:\n",
        "      queries: Queries (dict)\n",
        "      searcher: Pyserini object to perform retrieval\n",
        "      model_name: model name (string)\n",
        "\n",
        "    \"\"\"\n",
        "    run_bm25 = open(\"/content/run_BM25_{}.txt\".format(dataset),'a')\n",
        "    run_reranker = open(\"/content/run_{}_{}.txt\".format(model_name, dataset),'a')\n",
        "\n",
        "    list_t5 = []\n",
        "    for id, query in tqdm(queries.items()):\n",
        "      \n",
        "        hits = searcher.search(query[0:1024], k=1000)\n",
        "        texts = hits_to_texts(hits)\n",
        "        query = Query(query)\n",
        "        reranked = reranker.rerank(query, texts)\n",
        "        reranked.sort(key=lambda x: x.score, reverse=True)\n",
        "\n",
        "        for idx in range(len(hits)):\n",
        "\n",
        "            run_bm25.write(str(id)+' Q0 '+ str(hits[idx].docid) + ' ' + str(idx+1) + ' ' + str(hits[idx].score) + ' BM25\\n')\n",
        "            run_reranker.write(str(id)+' Q0 '+ str(reranked[idx].metadata[\"docid\"]) + ' ' + str(idx+1) + ' ' + str(math.exp(reranked[idx].score) * 100) + ' ' + model_name+'\\n')\n",
        "\n",
        "    run_bm25.close()\n",
        "    run_reranker.close()\n",
        "\n",
        "\n",
        "def evaluation(dataset, df_final, model_name):\n",
        "    \"\"\"\n",
        "    Run evaluation and prepare the dataframe results\n",
        "\n",
        "    Args:\n",
        "      dataset: Dataset name (string)\n",
        "      df_final: Dataframe containing final results (dataframe) \n",
        "      model_name: model name (string)   \n",
        "\n",
        "    Returns:\n",
        "      Returns the df_final dataframe containing run results\n",
        "    \"\"\"\n",
        "    for model in ['BM25', model_name]: \n",
        "        list_results = []\n",
        "        results = !python -m pyserini.eval.trec_eval -c -m all_trec /content/qrel.tsv /content/run_{model}_{dataset}.txt  \n",
        "        list_results.append(model)\n",
        "        list_results.append(dataset)      \n",
        "        for result in results:\n",
        "            line = result.split('\\t')\n",
        "            if len(line) == 3:\n",
        "                metric_name, _, value = line\n",
        "                metric_name = metric_name.strip()\n",
        "                if metric_name in metrics_map:\n",
        "                    list_results.append(value)\n",
        "\n",
        "        df_res = pd.DataFrame([list_results], columns = ['Model', 'Dataset', 'mAP', 'MRR', 'nDCG@5', 'nDCG@10'])\n",
        "        df_final = pd.concat([df_final, df_res])\n",
        "    \n",
        "    df_final.to_csv('BEIR_results.csv', index = False)\n",
        "    return df_final"
      ],
      "metadata": {
        "id": "ea7Zno34_6KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'monot5-small' #@param [\"monot5-small\", \"monot5-base\", \"monot5-3B\", \"MiniLM\"]"
      ],
      "metadata": {
        "id": "5u5Fb2Opz43S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## List of datasets to be evaluated \n",
        "datasets = [ \"trec-covid\", \"nfcorpus\", \"fiqa\", \"scifact\", \"webis-touche2020\", \"dbpedia-entity\", \"scidocs\", \"arguana\", \n",
        "                  \"climate-fever\", \"quora\", \"nq\", \"fever\", \"hotpotqa\"]\n",
        "\n",
        "## Download model\n",
        "if model_name == \"MiniLM\":\n",
        "    reranker = SentenceTransformersReranker(pretrained_model_name_or_path='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "else:\n",
        "    reranker = MonoT5(pretrained_model_name_or_path='castorini/{}-msmarco-10k'.format(model_name), token_false='▁false', token_true ='▁true')\n",
        "\n",
        "## Desired metrics\n",
        "metrics_map = {\n",
        "    'recip_rank': 'MRR',\n",
        "    'ndcg_cut_5': 'nDCG@5',\n",
        "    'ndcg_cut_10': 'nDCG@10',\n",
        "    'map': 'mAP',\n",
        "}\n",
        "\n",
        "####### START EXPERIMENTS ##########\n",
        "if not os.path.exists('/content/BEIR_results.csv'):\n",
        "    df_final = pd.DataFrame()\n",
        "else:\n",
        "    df_final = pd.read_csv('/content/BEIR_results.csv')\n",
        "\n",
        "for dataset in tqdm(datasets):\n",
        "    \n",
        "    # clear previous iteration files\n",
        "    !rm /content/qrel.tsv\n",
        "    clear_output(wait=True)\n",
        "    # Dowload data\n",
        "    corpus, queries, qrels = download_dataset(dataset)\n",
        "    # Index corpus\n",
        "    searcher = index_corpus(corpus)\n",
        "    # Convert qrels to TREC format\n",
        "    prepare_qrels(dataset)\n",
        "    # Run retrieval models\n",
        "    run_retrieval(queries, searcher, model_name)\n",
        "    # Evaluate\n",
        "    df_final = evaluation(dataset, df_final, model_name)"
      ],
      "metadata": {
        "id": "nJkGbrQdw1ba"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}