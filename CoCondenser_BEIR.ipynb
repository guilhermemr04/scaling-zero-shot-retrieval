{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_ZK_hkvCf1k"
      },
      "outputs": [],
      "source": [
        "!pip install beir\n",
        "!pip install jsonlines\n",
        "!pip install ir_measures==0.3.0 --quiet\n",
        "\n",
        "!git clone --recursive https://github.com/texttron/tevatron\n",
        "%cd tevatron\n",
        "!pip install --editable .\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo_Jv_H-Czvx"
      },
      "outputs": [],
      "source": [
        "from beir import util\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import jsonlines\n",
        "import pandas as pd\n",
        "import ir_measures\n",
        "from ir_measures import *\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def download_dataset(dataset):\n",
        "    \"\"\"\n",
        "    Download a BEIR dataset (test set only). Return the preprocessed corpus, queries and qrels\n",
        "\n",
        "    Args:\n",
        "      dataset: Dataset name (string)\n",
        "\n",
        "    Returns:  \n",
        "      Return the preprocessed corpus, queries and qrels\n",
        "    \"\"\"\n",
        "    print('Downloading', dataset)\n",
        "    url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
        "    data_path = util.download_and_unzip(url, \"datasets\")\n",
        "    corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
        "\n",
        "    return corpus, queries, qrels\n",
        "\n",
        "\n",
        "def tokenize_corpus(dataset, splits): \n",
        "    list_corpus = []\n",
        "    with jsonlines.open('/content/datasets/{}/corpus.jsonl'.format(dataset),'r') as reader:\n",
        "        for obj in reader:\n",
        "            list_corpus.append({'text_id':obj['_id'], 'title':obj['title'], 'text':obj['text']})\n",
        "            \n",
        "    df_corpus_dev = pd.DataFrame(list_corpus)\n",
        "    df_corpus_dev.to_csv('/content/corpus.tsv', sep='\\t', index=False, header=None)\n",
        "\n",
        "    !python /content/tevatron/examples/coCondenser-marco/tokenize_passages.py --tokenizer_name bert-base-uncased --file /content/corpus.tsv --save_to /content/corpus --n_splits {splits}\n",
        "\n",
        "\n",
        "def encode_corpus():\n",
        "    sh = \"\"\"\n",
        "    mkdir -p /content/encoding/corpus/\n",
        "    cd tevatron/src\n",
        "\n",
        "    for i in $(seq -f \"%02g\" 0 9)\n",
        "    do\n",
        "    python -m tevatron.driver.encode \\\n",
        "      --output_dir /content/ \\\n",
        "      --model_name_or_path Luyu/co-condenser-marco-retriever \\\n",
        "      --fp16 \\\n",
        "      --per_device_eval_batch_size 128 \\\n",
        "      --encode_in_path /content/corpus/split${i}.json \\\n",
        "      --encoded_save_path /content/encoding/corpus/split${i}.pt\n",
        "    done\n",
        "    cd /content/\n",
        "    \"\"\"\n",
        "    with open('script.sh', 'w') as file:\n",
        "      file.write(sh)\n",
        "\n",
        "    !bash script.sh\n",
        "    \n",
        "\n",
        "def tokenize_queries(dataset):\n",
        "    !mkdir query\n",
        "    list_queries = []\n",
        "    with jsonlines.open('/content/datasets/{}/queries.jsonl'.format(dataset),'r') as reader:\n",
        "        for obj in reader:    \n",
        "            list_queries.append({'query_id':obj['_id'], 'text':obj['text'][:900]})\n",
        "\n",
        "    f = open('/content/dev.query.txt','w')\n",
        "    for query in list_queries:\n",
        "        f.write(str(query['query_id']) + '\\t' + str(query['text']) + ' \\n')\n",
        "    f.close()\n",
        "\n",
        "    !python /content/tevatron/examples/coCondenser-marco/tokenize_queries.py --tokenizer_name bert-base-uncased --query_file /content/dev.query.txt --save_to /content/query/dev.query.json     \n",
        "\n",
        "\n",
        "def encode_queries():\n",
        "    !mkdir -p /content/encoding/query/\n",
        "    %cd tevatron/src\n",
        "\n",
        "    !python -m tevatron.driver.encode \\\n",
        "      --output_dir /content/ \\\n",
        "      --model_name_or_path Luyu/co-condenser-marco-retriever \\\n",
        "      --fp16 \\\n",
        "      --q_max_len 32 \\\n",
        "      --encode_is_qry \\\n",
        "      --per_device_eval_batch_size 128 \\\n",
        "      --encode_in_path /content/query/dev.query.json \\\n",
        "      --encoded_save_path /content/encoding/query/qry.pt\n",
        "\n",
        "\n",
        "def retrieval(dataset, k):\n",
        "    !python -m tevatron.faiss_retriever \\\n",
        "      --query_reps /content/encoding/query/qry.pt \\\n",
        "      --passage_reps /content/encoding/corpus/'*.pt' \\\n",
        "      --depth {k} \\\n",
        "      --batch_size -1 \\\n",
        "      --save_text \\\n",
        "      --save_ranking_to /content/rank_{dataset}.tsv\n",
        "    %cd /content/\n",
        "\n",
        "\n",
        "def prepare_qrels(dataset):\n",
        "    \"\"\"\n",
        "    Convert qreld to TREC eval format\n",
        "\n",
        "    Args:\n",
        "      dataset: Dataset name (string)\n",
        "\n",
        "    \"\"\"\n",
        "    df_qrel = pd.read_csv('/content/datasets/{}/qrels/test.tsv'.format(dataset), sep='\\t')\n",
        "    df_qrel['zero'] = '0'\n",
        "    cols = ['query-id', 'zero',\t'corpus-id', 'score']\n",
        "    df_qrel = df_qrel[cols] \n",
        "    df_qrel.to_csv('/content/qrel.tsv', sep='\\t', header = None, index = False)\n",
        "\n",
        "\n",
        "def prepare_topics(dataset):\n",
        "    df_rank = pd.read_csv('/content/rank_{}.tsv'.format(dataset), sep = '\\t', header = None)\n",
        "    run_reranker = open(\"/content/run_CoCondenser_{}.txt\".format(dataset),'a')\n",
        "\n",
        "    for query_id in df_rank[0].unique():\n",
        "        df_rankk = df_rank[df_rank[0]== query_id]\n",
        "        list_doc_ids = df_rankk[1].tolist()\n",
        "        list_score = df_rankk[2].tolist()\n",
        "        idx = 0\n",
        "        for doc_id, score in zip(list_doc_ids, list_score):\n",
        "            run_reranker.write(str(query_id)+' Q0 '+ str(doc_id) + ' ' + str(idx + 1) + ' ' + str(score) + ' CoCondenser\\n')\n",
        "            idx+=1\n",
        "\n",
        "    run_reranker.close()\n",
        "\n",
        "\n",
        "def evaluation(dataset, model_name):\n",
        "    \"\"\"\n",
        "    Run evaluation and prepare the dataframe results\n",
        "\n",
        "    Args:\n",
        "      dataset: Dataset name (string)\n",
        "      model_name: model name (string)   \n",
        "\n",
        "    Returns:\n",
        "      Returns the df_final dataframe containing run results\n",
        "    \"\"\"\n",
        "    ## Evaluation \n",
        "    run = ir_measures.read_trec_run('/content/run_{}_{}.txt'.format(model_name, dataset))\n",
        "    qrels = ir_measures.read_trec_qrels('/content/qrel.tsv')\n",
        "    result = ir_measures.calc_aggregate([nDCG@10], qrels, run)\n",
        "       \n",
        "    return result\n",
        "\n",
        "\n",
        "def delete_temp_data():\n",
        "    !rm -r /content/corpus\n",
        "    !rm /content/encoding/corpus/split.pt\n",
        "    !rm /content/encoding/query/qry.pt\n",
        "    !rm /content/query/dev.query.json\n",
        "    !rm /content/corpus.tsv\n",
        "    !rm /content/dev.query.txt\n",
        "    !rm /content/qrel.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRLeUDO9D6Fd"
      },
      "outputs": [],
      "source": [
        "model_name = \"CoCondenser\"  \n",
        "datasets = [\"trec-covid\",\"nfcorpus\",\"scifact\",\"scidocs\",\"fiqa\",\"arguana\",\"nq\",\"webis-touche2020\",\"quora\",\"dbpedia-entity\",\"climate-fever\",\"fever\",\"hotpotqa\"] \n",
        "\n",
        "list_results = []\n",
        "for dataset in datasets:\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Download dataset  \n",
        "    corpus, queries, qrels = download_dataset(dataset)\n",
        "\n",
        "    # Tokenize and encode corpus\n",
        "    tokenize_corpus(dataset, splits=10) \n",
        "    encode_corpus()\n",
        "\n",
        "    # Tokenize and encode queries\n",
        "    tokenize_queries(dataset)\n",
        "    encode_queries()\n",
        "\n",
        "    # Retrieval\n",
        "    retrieval(dataset, k=10)\n",
        "\n",
        "    # Evaluation \n",
        "    prepare_qrels(dataset)\n",
        "    prepare_topics(dataset)\n",
        "    result = evaluation(dataset, model_name)\n",
        "\n",
        "    # Save some previous results\n",
        "    list_results.append({'Dataset':dataset,'nDCG':result[nDCG@10]})\n",
        "    df_save = pd.DataFrame(list_results)\n",
        "    df_save.to_csv('/content/coCondenser-results.csv')\n",
        "    \n",
        "    # Delete temporary data created during execution\n",
        "    delete_temp_data()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CoCondenser_BEIR.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
