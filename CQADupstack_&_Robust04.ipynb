{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CQADupstack_&_Robust04.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall git+https://github.com/castorini/pygaggle\n",
        "!pip install faiss-cpu==1.7.2 --quiet\n",
        "!pip install jsonlines==3.0.0 --quiet\n",
        "!pip install beir==1.0.0 --quiet\n",
        "!pip install protobuf==3.20.1 --quiet\n",
        "!pip install ir_measures==0.3.0 --quiet\n",
        "!pip install ir-datasets==0.5.1 --quiet"
      ],
      "metadata": {
        "id": "RgKsEp4hKFe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4JBSI6k5y6N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available(): \n",
        "    dev = \"cuda:0\"\n",
        "    print(dev, torch.cuda.get_device_name(0))\n",
        "    device = torch.device(dev)\n",
        "else: \n",
        "    dev = \"cpu\"\n",
        "    print(dev) \n",
        "\n",
        "from pyserini.search import SimpleSearcher\n",
        "from pygaggle.rerank.base import hits_to_texts\n",
        "from pygaggle.rerank.base import Query, Text\n",
        "from pygaggle.rerank.transformer import MonoT5\n",
        "from transformers import T5ForConditionalGeneration\n",
        "from pygaggle.rerank.transformer import SentenceTransformersReranker\n",
        "import jsonlines\n",
        "import os\n",
        "import math\n",
        "from tqdm.notebook import tqdm\n",
        "from beir import util\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "import pandas as pd\n",
        "import ir_datasets\n",
        "import ir_measures\n",
        "from ir_measures import *\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_cqadupstack():\n",
        "    list_subsets = ['android','english','gaming','gis','mathematica','physics','programmers','stats','tex','unix','webmasters','wordpress']\n",
        "    list_queries = []\n",
        "    list_corpus = []\n",
        "    list_qrels = []\n",
        "\n",
        "    for subset in list_subsets:\n",
        "        dataset = ir_datasets.load(\"beir/cqadupstack/{}\".format(subset))\n",
        "\n",
        "        for query in dataset.queries_iter():\n",
        "            list_queries.append(query)\n",
        "\n",
        "        for doc in dataset.docs_iter():\n",
        "            list_corpus.append(doc)\n",
        "\n",
        "        for qrel in dataset.qrels_iter():\n",
        "            list_qrels.append(qrel)\n",
        "\n",
        "    return list_queries, list_corpus, list_qrels\n",
        "\n",
        "\n",
        "def download_robust04():\n",
        "    list_subsets = ['fold1','fold2','fold3','fold4','fold5']\n",
        "    list_queries = []\n",
        "    list_qrels = []\n",
        "\n",
        "    for subset in list_subsets:\n",
        "        dataset = ir_datasets.load(\"trec-robust04/{}\".format(subset))\n",
        "\n",
        "        for query in dataset.queries_iter():\n",
        "            list_queries.append(query)\n",
        "\n",
        "        for qrel in dataset.qrels_iter():\n",
        "            list_qrels.append(qrel)\n",
        "\n",
        "    return list_queries, list_qrels\n",
        "\n",
        "\n",
        "def index_corpus(corpus):\n",
        "    \"\"\"\n",
        "    Index corpus to be retrieved by BM25\n",
        "\n",
        "    Args:\n",
        "      corpus: Corpus (dict)\n",
        "      \n",
        "    Returns:\n",
        "      Searcher object to initialize BM25\n",
        "    \"\"\"\n",
        "\n",
        "    !rm -r candidates\n",
        "    !rm -r tmp_candidates\n",
        "    !mkdir tmp_candidates\n",
        "\n",
        "    for docs in corpus:  \n",
        "        indexed_dict = { \"id\": str(docs[0]), \"contents\": docs[2] + ' ' + docs[1]}\n",
        "        with jsonlines.open('/content/tmp_candidates/candidate.jsonl', mode='a') as writer:\n",
        "            writer.write(indexed_dict)\n",
        "         \n",
        "    !python -m pyserini.index -collection JsonCollection -generator DefaultLuceneDocumentGenerator \\\n",
        "    -threads 1 -input /content/tmp_candidates \\\n",
        "    -index /content/candidates/indexes -storePositions -storeDocvectors -storeRaw\n",
        "\n",
        "    searcher = SimpleSearcher('/content/candidates/indexes')\n",
        "\n",
        "    return searcher\n",
        "\n",
        "\n",
        "def prepare_qrels(list_qrels):\n",
        "    \"\"\"\n",
        "    Convert qreld to TREC eval format\n",
        "\n",
        "    Args:\n",
        "      list_qrels: list of qrels (list)\n",
        "\n",
        "    \"\"\"\n",
        "    list_qrels_trec = []\n",
        "    for qrels in list_qrels:\n",
        "        list_qrels_trec.append([str(qrels[0]), '0', str(qrels[1]), str(qrels[2])])\n",
        "    \n",
        "    cols = ['query-id', 'zero',\t'corpus-id', 'score']\n",
        "    df_qrel = pd.DataFrame(list_qrels_trec, columns= cols)\n",
        "  \n",
        "    df_qrel.to_csv('qrel.tsv', sep='\\t', header = None, index = False)\n",
        "\n",
        "\n",
        "def run_retrieval(list_queries, searcher, dataset, model_name):\n",
        "    \"\"\"\n",
        "    Run BM25 and reranker retrieval. Save the outputs as txt files\n",
        "\n",
        "    Args:\n",
        "      queries: Queries (dict)\n",
        "      searcher: Pyserini object to perform retrieval\n",
        "      dataset: dataset name (string)\n",
        "      model_name: model name (string)\n",
        "\n",
        "    \"\"\"\n",
        "    run_bm25 = open(\"/content/run_BM25_{}.txt\".format(dataset),'a')\n",
        "    run_reranker = open(\"/content/run_{}_{}.txt\".format(model_name, dataset),'a')\n",
        "\n",
        "    list_t5 = []\n",
        "    for query in tqdm(list_queries):\n",
        "      \n",
        "        hits = searcher.search(query[1][0:1024], k=1000)\n",
        "        texts = hits_to_texts(hits)\n",
        "        query_ = Query(query[1])\n",
        "        reranked = reranker.rerank(query_, texts)\n",
        "        reranked.sort(key=lambda x: x.score, reverse=True)\n",
        "\n",
        "        for idx in range(len(hits)):\n",
        "\n",
        "            run_bm25.write(str(query[0])+' Q0 '+ str(hits[idx].docid) + ' ' + str(idx+1) + ' ' + str(hits[idx].score) + ' BM25\\n')\n",
        "            run_reranker.write(str(query[0])+' Q0 '+ str(reranked[idx].metadata[\"docid\"]) + ' ' + str(idx+1) + ' ' + str(math.exp(reranked[idx].score) * 100) + ' ' + model_name+'\\n')\n",
        "\n",
        "    run_bm25.close()\n",
        "    run_reranker.close()\n",
        "\n",
        "\n",
        "def evaluation(dataset, model_name):\n",
        "    \"\"\"\n",
        "    Run evaluation and prepare the dataframe results\n",
        "\n",
        "    Args:\n",
        "      dataset: Dataset name (string)\n",
        "      model_name: model name (string)   \n",
        "\n",
        "    Returns:\n",
        "      Returns the df_final dataframe containing run results\n",
        "    \"\"\"\n",
        "    ## Evaluation \n",
        "    run_bm25 = ir_measures.read_trec_run('/content/run_BM25_{}.txt'.format(dataset))\n",
        "    qrels = ir_measures.read_trec_qrels('/content/qrel.tsv')\n",
        "    result_bm25 = ir_measures.calc_aggregate([nDCG@10], qrels, run_bm25)\n",
        "    \n",
        "    ## Evaluation \n",
        "    run = ir_measures.read_trec_run('/content/run_{}_{}.txt'.format(model_name, dataset))\n",
        "    qrels = ir_measures.read_trec_qrels('/content/qrel.tsv')\n",
        "    result = ir_measures.calc_aggregate([nDCG@10], qrels, run)\n",
        "       \n",
        "    return result_bm25, result"
      ],
      "metadata": {
        "id": "7PxCX51zKErY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'MiniLM' #@param [\"monot5-small\", \"monot5-base\", \"monot5-3B\", \"MiniLM\"]"
      ],
      "metadata": {
        "id": "Irxq4D7COhaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## List of datasets to be evaluated \n",
        "datasets = ['cqadupstack', 'robust04']\n",
        "\n",
        "## Download model\n",
        "if model_name == \"MiniLM\":\n",
        "    reranker = SentenceTransformersReranker(pretrained_model_name_or_path='cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "else:\n",
        "    reranker = MonoT5(pretrained_model_name_or_path='castorini/{}-msmarco-10k'.format(model_name), token_false='▁false', token_true ='▁true')\n",
        "\n",
        "list_results = []\n",
        "for dataset in tqdm(datasets):\n",
        "    \n",
        "    \n",
        "    !rm /content/qrel.tsv\n",
        "    if dataset == 'cqadupstack':\n",
        "        # Dowload data\n",
        "        list_queries, list_corpus, list_qrels = download_cqadupstack()\n",
        "        # Index corpus\n",
        "        searcher = index_corpus(list_corpus)\n",
        "    else:\n",
        "        # Dowload data\n",
        "        list_queries, list_qrels = download_robust04()\n",
        "        # Load indexed corpus\n",
        "        searcher = SimpleSearcher.from_prebuilt_index(dataset)\n",
        "\n",
        "    # Run retrieval models\n",
        "    run_retrieval(list_queries, searcher, dataset, model_name)\n",
        "    # prepare qrels\n",
        "    prepare_qrels(list_qrels)\n",
        "    # Evaluate\n",
        "    result_bm25, result = evaluation(dataset, model_name)\n",
        "    list_results.append((dataset, list(result_bm25.values())[0], list(result.values())[0]))\n",
        "\n",
        "    # clear previous iteration files\n",
        "    clear_output(wait=True)\n",
        "\n",
        "pd.DataFrame(list_results, columns = ['Dataset', 'BM25', 'Reranker']).to_csv('BEIR_results.csv', index = False)"
      ],
      "metadata": {
        "id": "tbW-wOcHPywy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}